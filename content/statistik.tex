\renewcommand{\N}{\mathcal{N}}
\renewcommand{\K}{\mathcal{K}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\uiv}{\stackrel{\text{uiv}}{\sim}}

\subsection*{Normalverteilung}

Dichte der Normalverteilung \(\N(\mu, \sigma^2)\):
\[ f(t) := \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(t-\mu)^2}{2\sigma^2} \right) \]

Verteilungsfkt. der Normalverteilung:
\[ \Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x \exp\left(-\frac{1}{2} t^2\right) dt \]

\subsection*{Gamma-Verteilung}

Dichte der Verteilung \(\Gamma(\alpha,\beta)\) für \(\alpha,\beta > 0\):
\[ f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} \exp(-\beta x) x^{\alpha-1} \1_{(0,\infty)}(x) \]
Mit Momenten:
\[ EX^r = \frac{\Gamma(\alpha + r)}{\beta^r \Gamma(\alpha)} \text{ für } r > -\alpha \]
Insb. also \(EX = \frac{\alpha}{\beta}\).

\(cX \sim \Gamma(\alpha,\frac{\beta}{c})\) für \(c > 0\) und \(X \sim \Gamma(\alpha,\beta)\).

Faltungsformel für \(X \sim \Gamma(\alpha_1,\beta), Y \sim \Gamma(\alpha_2,\beta)\):
\[ X + Y \sim \Gamma(\alpha_1+\alpha_2, \beta) \]

\subsubsection*{Gamma-Funktion}

\[ \Gamma(t) = \int_0^\infty \exp(-x) x^{t-1} dx \text{ für } t > 0 \]

Insb. \(\Gamma(t+1) = t\Gamma(t)\) und \(\Gamma(n+1) = n!\) für \(n \in \mathbb{N}_0\).

\subsection*{\(\chi^2\)-Verteilung}

Sei \(N_1,\dots,N_k \uiv \N(0,1)\).
\[ Y:=N_1^2+\dots+N_k^2 \in \chi_k^2 \]
Mit Dichte für \(y > 0\):
\[ f(y) = \frac{1}{2^{k/2} \Gamma(k/2)} \exp\left(-\frac{y}{2}\right) y^{k/2-1} \]
Und Erwartungswert (\(EY^r = \infty\) für \(r\leq -k/2\)):
\[ EY^r = \frac{2^r \Gamma(r+k/2)}{\Gamma(k/2)} \text{ für } r > -\frac{k}{2} \]
Insb. also \(EY = k\) und \(VY = 2k\).

\subsection*{\(t\)-Verteilung}

Sei \(N \sim \N(0,1)\) unabhg. \(X \sim \chi_k^2\).
\[ Y := \frac{N}{\sqrt{X/k}} \sim t_k \]
Mit \(EY = 0\) für \(k \geq 2\) und \(VY=\frac{k}{k-2}\) für \(k \geq 3\).

\subsection*{\(F\)-Verteilung}

Sei \(R \sim \chi_r^2\) unabhg. \(S \sim \chi_s^2\).
\[ Y:= \frac{\frac{1}{r} R}{\frac{1}{s} S} \sim F_{r,s} \]

\subsection*{Starkes Gesetz großer Zahlen (SGGZ)}

Sei \(Y_1,Y_2,\dots\) Folge uiv. ZV mit EW, dann:
\[ \frac{1}{n} \sum_{i=1}^n Y_i \to EY_1 \ \text{(P-fast sicher)} \]

\subsection*{Zentraler Grenzwertsatz (ZGWS)}

Sei \((X_n)_{n\geq1}\) Folge uiv. ZV mit \(0 < \sigma^2 = V(X_1) < \infty\) und \(\mu = EX_1\). Dann gilt für \(-\infty \leq a < b \leq \infty\):
\[ P\left( a \leq \frac{\sqrt{n}(\overline X_n - \mu)}{\sigma} \leq b \right) \to \Phi(b) - \Phi(a) \ (n \to \infty) \]

\section*{Maximum-Likelihood-Schätzer}

Sei \(X_1, \dots, X_n\) uiv. ZV und \(\upsilon\) Modellparameter.
\[ L_x(\upsilon) = P_\upsilon(X=x) = \prod_{i=1}^n f(x_i, \upsilon) \]

Die Log-Likelihood-Funktion:
\[ \ell_x(\upsilon) := \log L_x(\upsilon) = \sum_{i=1}^n \log f(x_i, \upsilon) \]

Maximierendes \(\hat\upsilon\) ist MLS.

\subsection*{ML-Schätzer bei NV-Annahme}

Sei \(X_1, \dots, X_n \sim \N(\mu,\sigma^2)\) uiv. ZV, \(\upsilon = (\mu,\sigma^2) \in \Theta\)
\[ L_x(\upsilon) = \frac{1}{\sqrt{2\pi\sigma^2}^n} \exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i-\mu)^2 \right) \]
\[ \ell_x(\upsilon) = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i-\sigma)^2 \]

ML-Schätzer für \(\hat\mu\):
\[ \hat\mu = \overline X = \frac{1}{n}\sum_{i=1}^n X_i \]

ML-Schätzer für \(\hat\sigma^2\):
\[ \hat\sigma^2 = \frac{1}{n}\sum_{i=1}^n (X_i-\overline X)^2 \]

\section*{Momentenmethode}

Modellparam. als Fkt. der empirischen Momente:
\[ \hat m_\ell := \frac{1}{n} \sum_{j=1}^n X_j^\ell \]

z.B. Varianz als Fkt. der Momente:
\[ VX = EX^2 - (EX)^2 = \hat m_2 - (\hat m_1)^2 \]

\section*{Eigenschaften von Schätzern}

Sei \(T : \Xi \to \Sigma\) Schätzer für \(\gamma(\upsilon)\) und es gelte \(\forall \upsilon \in \Theta : E_\upsilon(T^2) < \infty\). Dann:
\begin{align*}
\text{MQA}_T(\upsilon) :&= E_\upsilon(T-\gamma(\upsilon))^2 \\
&= V_\upsilon(T) + b_T^2(\upsilon)
\end{align*}

Wobei die \emph{Verzerrung} def. ist als:
\[ b_T(\upsilon) := E_\upsilon T(X) - \gamma(\upsilon) \]

Schätzer \(T\) ist \emph{erwartungstreu} für \(\gamma(\upsilon)\), wenn:
\[ \forall \upsilon \in \Theta : E_\upsilon T(X) = \gamma(\upsilon) \]

Schätzfolge \((T_n)_{n \in \mathbb{N}}\) ist \emph{konsistent}, wenn:
\[ \forall \epsilon > 0, \upsilon \in \Theta : \lim_{n\to\infty} P_\upsilon(|T_n - \gamma(\upsilon)| \geq \epsilon) = 0 \]

Schätzfolge \((T_n)_{n \in \mathbb{N}}\) ist \emph{asympt.-EW-treu}, wenn:
\[ \forall \upsilon \in \Theta : \lim_{n\to\infty} E_\upsilon(T_n) = \gamma(\upsilon) \]

Ist eine Schätzfolge asympt.-EW-treu und gilt \(V_\upsilon(T_n) \to 0 \ (n\to\infty)\), so ist sie konsistent.

\subsection*{Scorefunktion}

\[U_\upsilon(X_1) := \partial_\upsilon \log f(X_1,\upsilon) \]

Diese hat EW: \(E_\upsilon(U_\upsilon(X_1)) = 0\).

\subsubsection*{Fisher-Information}

Die Varianz der Scorefunktion:
\[ I(\upsilon) = V_\upsilon(U_\upsilon) = E_\upsilon(U_\upsilon^2) = -E\left[ \partial_\upsilon^2 \log f(X_1,\upsilon) \right]\]

\subsubsection*{Score-Gleichung}

Notwendige Bedingung für ML-Schätzer \(\hat\upsilon\):
\[\sum_{i=1}^n \partial_\upsilon \log f(x_i,\upsilon) = 0 \]

Für jede konsistente Folge von Lsg. dieser Gl. gilt die Verteilungskonvergenz:
\[ \sqrt{n}(\hat\upsilon_n - \upsilon_0) \to \N(0, 1/I(\upsilon_0)) \]

\subsubsection*{Asymptotische Effizienz}

Sei \((T_n)_{n \in \mathbb{N}}\) Schätzfolge für \(\upsilon_0\) mit:
\[\sqrt{n}(T_n - \upsilon_0) \to \N(0,\sigma^2) \]
Dann ist die \emph{asymptotische Effizienz} geg. als:
\[ e(T_n) = \frac{1/I(\upsilon_0)}{\sigma^2} \]
Schätzfolge ist \emph{asymptotisch effizient} für \(e(T_n)=1\).

\subsection*{Cram\'er-Rao-Ungleichung}

Erfüllen \(X_1,\dots,X_n \sim f(x,\upsilon)\) die Regularitätsbed.:
\begin{enumerate}
	\item \(\Theta\) ist offenes Intervall in \(\R\)
	\item Träger \(\{x \in \Xi | f(x,\upsilon) > 0\) unabhg. \(\upsilon\)
	\item \(\forall x \in \Xi : f(x,\upsilon)\) zweimal nach \(\upsilon\) diffbar
	\item \(\int f(x,\upsilon) dx\) zweimal im Int. nach \(\upsilon\) diffbar
\end{enumerate}
Sei \(T(X_1,\dots,X_n\) Schätzer für \(\gamma(\upsilon)\) mit zweimal unter Int. db. EW \(k(\upsilon) := E_\upsilon(T)\) und \(E_\upsilon(T^2) < \infty\). Dann:
\[ V_\upsilon(T) \geq \frac{(k'(\upsilon))^2}{nI(\upsilon)} \]

\subsubsection*{Cram\'er-Rao Effizienz}

Schätzer \(T\) nimmt Cram\'er-Rao-Schranke an.

\section*{Konfidenzintervalle}

Sei \(X_1,\dots,X_n \uiv \N(\mu,\sigma^2)\) für bekannte \(\sigma^2\):
\[ I := \left[ \overline x_n - \frac{\sigma z_{1-\alpha/2}}{\sqrt{n}}, \overline x_n + \frac{\sigma z_{1-\alpha/2}}{\sqrt{n}} \right] \]
Dann \(P_\mu(\mu \in I) = 1-\alpha\).

\subsection*{Konstruktionsprinzip}

\begin{enumerate}
	\item Finde \emph{Pivot} ZV \(\Z\) unabhg. \(\upsilon\)
	\item Bestimme \(a, b\) s.d. \(P(a \leq Z \leq b) = 1-\alpha\)
	\item Löse zu gesuchtem \(g(\upsilon)\) auf
\end{enumerate}

\subsection*{Wald-Intervall}

Sei \(X_1,\dots,X_n \uiv \text{Bin}(1,p)\) und \(\hat p_n = \frac{1}{n}\sum_{i=1}^n X_i\).

Approximatives Konfidenzintervall für \(p\):
\[ \left[\hat p_n - \frac{z_{1-\frac{\alpha}{2}}}{\sqrt{n}} \sqrt{\hat p_n (1-\hat p_n)}, \hat p_n + \frac{z_{1-\frac{\alpha}{2}}}{\sqrt{n}} \sqrt{\hat p_n (1-\hat p_n)}\right] \]

\section*{Satz von Student}

Sei \(X_1,\dots,X_n \uiv \N(\mu,\sigma^2)\) für \(n \geq 2\). Dann gelten:
\[ \overline X = \frac{1}{n}\sum_{i=1}^n X_i \sim \N\left(\mu,\frac{\sigma^2}{n}\right) \]

\(\overline X\) ist unabhg. \(S^2 := \frac{1}{n-1} \sum_{i_1}^n (X_i-\overline X)^2\).

\[ \frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2 \]

\[ T = \frac{\sqrt{n}(\overline X - \mu)}{S} \sim t_{n-1} \]

\subsection*{Konfidenzintervall für \(\mu\)}

Sei \(X_1,\dots,X_n \uiv \N(\mu,\sigma^2\) für unbekannte \(\mu, \sigma^2\).
\[ P_\upsilon\left(\left|\frac{\sqrt{n}(\overline X - \mu)}{S}\right| \leq t_{n-1;1-\alpha/2}\right) = 1-\alpha \]

\[ \mu \in \left[\overline X - \frac{S}{\sqrt{n}} t_{n-1;1-\alpha/2}, \overline X + \frac{S}{\sqrt{n}} t_{n-1;1-\alpha/2} \right] \]

\section*{Tests}

Test von Hypothese \(H_0\) gegen Alternative \(H_1\).

Fehler 1. Art: \(H_0\) gilt, wird aber abgelehnt.

Fehler 2. Art: \(H_1\) gilt, \(H_0\) wird nicht verworfen.

Niveau \(\alpha\) kontrolliert WKeit von Fehler 1. Art.

Zerlege Param. von \((\Xi, (P_\upsilon)_{\upsilon \in \Theta})\) in \(\Theta = \Theta_0 \dot\cup \Theta_1\).

Kritischer Bereich \(\K \subset \Xi\) gibt nichtrandom. Test.

\(x \in \K \implies H_1 \text{ gilt}\), \(x \in \Xi \setminus \K \implies H_0 \text { gilt}\)

\subsection*{\(z\)-Test}

Unter \(H_0\) gilt \(\overline X_n \sim \N(\mu_0,\sigma^2/n)\) für bekanntes \(\sigma^2\).

Testgröße ist \(T = \frac{\sqrt{n}(\overline X_n - \mu_0)}{\sigma} \sim \N(0,1)\)

Lehne \(H_0\) ab für \(T \leq z_\alpha = \Phi^{-1}(\alpha)\).

\subsection*{Gütefunktion}

\[ g(\upsilon) := P_\upsilon(X \in \K) = P_\upsilon(\text{Lehne \(H_0\) ab}) \]

Ideal: \(\forall \upsilon \in \Theta_0 : g(\upsilon) = 0\) und \(\forall \upsilon \in \Theta_1 : g(\upsilon) = 1\).

\(\sup_{\upsilon \in \Theta_0} g(\upsilon)\) ist \emph{Umfang} des Tests. Dieser soll möglichst nahe bei Niveau \(\alpha\) liegen.

\subsection*{Ein-Stichproben-\(t\)-Test}

Teste \(H_0 : \mu = \mu_0\) gegen \(H_1 : \mu \neq \mu_0\)

Testgröße \(T(x_1,\dots,x_n) = \frac{\sqrt{n}(\overline x - \mu_0)}{S}\) (vgl. Student)

Verwerfe \(H_0\) für \(|T| \geq t_{n-1;1-\alpha/2}\)

\subsection*{Ein-Stichproben-Varianz-Test}

Sei \(\chi^2 := \frac{(n-1)S^2}{\sigma_0^2} \sim \chi_{n-1}^2\) Testgröße.

Teste \(H_0 : \sigma^2 = \sigma_0^2\) gegen \(H_1 : \sigma^2 > \sigma_0^2\):

Verwerfe \(H_0\) für \(\chi^2 \geq \chi_{n-1;1-\alpha}^2\)

\spacing

Teste \(H_0 : \sigma^2 = \sigma_0^2\) gegen \(H_1 : \sigma^2 < \sigma_0^2\):

Verwerfe \(H_0\) für \(\chi^2 \leq \chi_{n-1;1-\alpha}^2\)

\subsection*{\(p\)-Wert}

WKeit unter \(H_0\) etwas mindestens so Extremes zu beobachten, wie das tatsächlich Beobachtete.
\[ p^\ast = P_{H_0}(T \geq T(x)) \]

\(p^\ast \leq \alpha \implies H_0\) wird auf Niveau \(\alpha\) verworfen

\(p^\ast\) ist min. \(H_0\)-verwerfendes Signifikanzniveau.

\subsection*{Bester Test}

Ein \emph{bester} Test für \(H_0 : \upsilon \in \Theta_0\) gegen \(H_1 : \upsilon \in \Theta_1\) ist ein Niveau-\(\alpha\)-Test s.d. \(g(\upsilon)\) maximal \(\forall \upsilon \in \Theta_1\).

\subsubsection*{Neyman-Pearson-Lemma}

Sei \(f_0\), \(f_1\) Dichte unter \(H_0\) bzw. \(H_1\).
\[ h_k(x) = \prod_{i=1}^n f_k(x_i) \text{ für } k \in \{0,1\} \]

Neyman-Pearson-Test mit Testentscheid:

\(H_0\) verwerfen für \(h_1(x) \geq c \cdot h_0(x)\).

\(H_1\) nicht verwerfen für \(h_1(x) < c \cdot h_0(x)\).

Ist bester Test für \(H_0 : \upsilon = \upsilon_0\) gegen \(H_1 : \upsilon = \upsilon_1\).

d.h. Test-Statistik: \(T(x) = \frac{h_1(x)}{h_0(x)} \geq c\)

\subsection*{Likelihood-Quotienten-Tests}

Test von \(H_0 : \upsilon \in \Theta_0\) gegen \(H_1 : \upsilon \in \Theta \setminus \Theta_0\)

\[ \Lambda(x) := \frac{\sup_{\upsilon \in \Theta} L_x(\upsilon)}{\sup_{\upsilon \in \Theta_0} L_x(\upsilon)} \geq 1 \]

Verwerfe \(H_0\) für \emph{große} Werte von \(\Lambda\).

Insb. teste für offene Intervalle \(\Theta \subset \R\) die Hypothese \(H_0 : \upsilon = \upsilon_0\) gegen \(H_1 : \upsilon \neq \upsilon_0\) mit LQ-Statistik für ML-Schätzwert \(\hat\upsilon_n\):
\[ \Lambda_n = \frac{L_x(\hat\upsilon_n)}{L_x(\upsilon_0)} = \prod_{i=1}^n \frac{f(x_i,\hat\upsilon_n)}{f(x_i,\upsilon_0)} \]

\subsubsection*{Verteilung der LQ-Testgröße}

Sei \(X_1,X_2,\dots\) Folge uiv. ZV mit Dichte \(f(x,\upsilon_0)\), erfüllten Regularitätsbed. und \(I(\upsilon_0) \in (0,\infty)\). Dann gilt für die ML-Schätzfolge \(\hat\upsilon_n\):
\[ \sqrt{n}(\hat\upsilon_n - \upsilon_0) \to \N(0,1/I(\upsilon_0)) \]

Und für die Folge der LQ-Statistiken \((\Lambda_n(X))_{n \in \mathbb{N}}\):
\[ 2 \log(\Lambda_n) \to \chi_1^2 \]

Es ergibt sich so der Testentscheid:

\(H_0\) verwerfen für \(2 \log(\Lambda_n) \geq \chi_{1;1-\alpha}^2\)

\subsection*{Lagevergleich unabhg. Stichproben}

\(X_1,\dots,X_m \sim \N(\mu,\sigma^2)\) unbg. \(Y_1,\dots,Y_n \sim \N(\nu,\tau^2)\).
\(\implies \overline X_m \sim \N(\mu,\sigma^2, \ \overline Y_n \sim \N(\nu,\tau^2/n)\)

\[ \frac{(m-1)S_X^2}{\sigma^2} \sim \chi_{m-1}^2, \ \frac{(n-1)S_Y^2}{\tau^2} \sim \chi_{n-1}^2 \]

Aus der Faltungsformel folgt unter \(\sigma^2 = \tau^2\):
\[ \overline X_m - \overline Y_n - (\mu - \nu) \sim \N\left(0,\frac{m+n}{mn} \sigma^2\right) \]

\vspace*{-5mm}
\begin{align*}
S_{m,n}^2 :&= \frac{1}{m+n-2} \left((m-1)S_X^2 + (n-1)S_Y^2\right) \\
&\frac{(m+n-2)S_{m,n}^2}{\sigma^2} \sim \chi_{m+n-2}^2 \\
&\sqrt{\frac{mn}{m+n}}\frac{\left(\overline X_m - \overline Y_n - (\mu-\nu)\right)}{S_{m,n}} \sim t_{m+n-2}
\end{align*}

\[ P\left( \sqrt{\frac{mn}{m+n}} \frac{|\overline X_m - \overline Y_n - (\mu-\nu)|}{S_{m,n}} \leq t_{m+n-2;1-\frac{\alpha}{2}} \right) = 1-\alpha \]

\subsection*{Zwei-Stichproben-\(t\)-Test}

Teste \(H_0 : \mu = \nu\) gegen \(H_1 : \mu \neq \nu\) unter \(\sigma^2=\tau^2\) mit unter \(H_0\) \(t\)-verteilter Testgröße:
\[ T_{m,n} := \sqrt{\frac{mn}{m+n}} \frac{\overline X_m - \overline Y_n}{S_{m,n}}a \sim t_{m+n-2} \]

Der Testentscheid lautet also:

\(H_0\) verwerfen für \(|T_{m,n}| \geq t_{m+n-2;1-\frac{\alpha}{2}}\)

\subsection*{\(F\)-Test für den Varianzquotienten}

Teste \(H_0 : \sigma^2 = \tau^2\) gegen \(H_1 : \sigma^2 \neq \tau^2\):
\[ Q_{m,n} := \frac{\frac{1}{m-1}\sum_{i=1}^m(X_i-\overline X_m)^2}{\frac{1}{n-1}\sum_{i=1}^m(Y_i-\overline Y_n)^2} \sim F_{m-1,n-1} \]

Verwerfe \(H_0\) für große und kleine Testwerte, d.h.: \(Q_{m,n} \leq F_{m-1,n-1;\frac{\alpha}{2}}\) oder \(Q_{m,n} \geq F_{m-1,n-1;1-\frac{\alpha}{2}}\)
